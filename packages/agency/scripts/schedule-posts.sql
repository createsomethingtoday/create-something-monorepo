INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_7xctrpgkvef', 'linkedin', 'New research paper: The Norvig Partnership

Peter Norvig—author of "Artificial Intelligence: A Modern Approach"—spent December 2025 working through Advent of Code with LLM assistance.

His findings validate what phenomenology predicted about AI-human collaboration.

The empirical data:
- "Maybe 20 times faster" than manual coding
- Correct answers to every puzzle
- LLMs demonstrated mastery of professional CS concepts without explicit instruction

The phenomenological insight:
When Norvig concluded "I should use an LLM as an assistant for all my coding, not just as an experiment," he marked the Zuhandenheit moment—when a tool recedes so completely from attention that it becomes inseparable from practice itself.

The methodology validation:
Norvig''s pattern—human direction + AI execution + rapid corrective feedback—is precisely what CREATE SOMETHING''s harness architecture implements. His empirical discovery converges with our phenomenological predictions.

Key takeaways for practitioners:
1. Partnership, not replacement. The AI doesn''t work alone.
2. Bounded tasks. "Build auth system" fails. "Add JWT to login endpoint" succeeds.
3. Corrective feedback. Quick breakdown-repair cycles build trust.
4. The tool disappears. When it works, you stop evaluating and start using.

Read the full analysis linking Norvig''s empirics to Heideggerian phenomenology.', 1767373200000, 'pending', 'Unified Thesis - Gateway (Company Page)', 1767275140446, '{"organizationId":"110433670","commentLink":"Full paper: createsomething.io/papers/norvig-partnership\n\nNorvig''s original notebook: github.com/norvig/pytudes/blob/main/ipynb/Advent-2025-AI.ipynb\n\n#AIResearch #Phenomenology #SoftwareEngineering"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_bp7fq8241d6', 'linkedin', 'Peter Norvig just changed how I think about AI coding assistants.

The author of "Artificial Intelligence: A Modern Approach"—the definitive AI textbook—spent December working through Advent of Code 2025 with LLM assistance. His findings:

- "Maybe 20 times faster" than manual coding
- Correct answers to every puzzle
- LLMs demonstrated mastery of professional CS concepts without explicit instruction

But the real insight isn''t the speed. It''s his conclusion:

"I''m beginning to think I should use an LLM as an assistant for all my coding, not just as an experiment."

This is the moment when a tool stops being something you evaluate and becomes something you work through. Heidegger called this Zuhandenheit—ready-to-hand. The hammer disappears when you''re hammering. The LLM disappears when you''re coding.

What enabled this shift?

Partnership, not replacement. Norvig didn''t hand problems to the AI and walk away. When Day 1 Part 2 failed, he diagnosed the issue, provided corrective feedback, and the AI adjusted. The pattern repeated: human judgment + AI execution.

The AI wasn''t autonomous. It was collaborative. The human retained authority over what to build, how to evaluate correctness, when something was good enough. The AI handled translation from problem to code.

This validates what we''ve been building: systems where AI does the work and humans design the environment that makes work possible. Context files. Bounded tasks. Persistent memory across sessions.

The question isn''t "can AI code?" It''s "how do we work with AI?"

Norvig''s answer, arrived at empirically: partnership.', 1767632400000, 'pending', 'Unified Thesis - Gateway', 1767275140446, '{"commentLink":"Full analysis: createsomething.io/papers/norvig-partnership\n\nNorvig''s original notebook: github.com/norvig/pytudes/blob/main/ipynb/Advent-2025-AI.ipynb\n\n#AIPartnership #ClaudeCode #SoftwareEngineering"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_yws5sewn7ki', 'linkedin', 'I''ve been posting scattered content for months. Papers, LMS promos, experiments, random thoughts. Look at my recent activity—it''s all over the place.

It diluted the message. No unified thesis. No clear entry point. Just fragments hoping something would stick.

Here''s the new approach.

Three accounts, three purposes:

**CREATE SOMETHING** posts research. Papers, experiments, methodology. The analytical layer. "Here''s what we''ve found."

**WORKWAY** posts practice. Building-in-public, decisions made with the methodology. The applied layer. "Here''s how we use it."

**This account** synthesizes. I repost from both, add context, connect insights to my daily work at Webflow. The hermeneutic layer. "Here''s what it means."

Why restructure? Because Dieter Rams'' principle—"less, but better"—applies to LinkedIn itself. Each account should serve one purpose. Mine was trying to serve all three. Now it curates instead of originates.

The unified thesis across all three:

AI development works when you treat LLMs as partners, not tools. Peter Norvig proved it empirically. We''ve built the methodology. I''m learning it through daily practice.

Starting now: CREATE SOMETHING posted a paper analyzing Norvig''s Advent of Code experiments. I''ll share my practitioner''s take on it shortly.

If you''ve followed me for the scattered content—sorry. If you''re here for the partnership thesis—welcome.

Less, but better.', 1767718800000, 'pending', 'Unified Thesis - Strategy Announcement', 1767275140446, '{"commentLink":"The gateway paper: createsomething.io/papers/norvig-partnership\n\nFollow CREATE SOMETHING for research, WORKWAY for practice.\n\n#Transparency #AIPartnership #LessButBetter"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_nvhhc49f5r7', 'linkedin', 'WORKWAY is built differently.

We''re documenting every decision through a methodology called the Subtractive Triad. Before adding anything, we ask three questions:

1. **Have I built this before?** (DRY) → Unify.
2. **Does this earn its existence?** (Rams) → Remove.
3. **Does this serve the whole?** (Heidegger) → Reconnect.

Most businesses add features hoping something sticks. We remove everything that doesn''t earn its place.

Example: We considered adding a "Our Process" page. Eight steps, fancy graphics, the works.

Question 1: Have I built this before? Yes—we already describe the process in the Services section.
Question 2: Does a separate page earn its existence? No. Same content, different URL. Redundancy.

Rejected at Level 2. No "Our Process" page.

This isn''t minimalism for aesthetics. It''s operational clarity. Visitors shouldn''t click around trying to find information. They should find it immediately or not at all.

The template we''re using is built on CREATE SOMETHING''s Canon design system. Pure black and white. No color to distract. Typography carries the hierarchy.

Why share this publicly? Because the methodology only works if we practice it ourselves. WORKWAY is the test case for everything CREATE SOMETHING researches.

We''re building in public. Follow along.', 1767805200000, 'pending', 'WORKWAY Launch', 1767275140446, '{"organizationId":"35463531","commentLink":"Live site: workwayarchitects.createsomething.space\n\nThe methodology behind the decisions: createsomething.ltd/ethos\n\n#BuildingInPublic #DesignDecisions #SubtractiveTriad"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_idqygmsa9z', 'linkedin', 'Most teams treat AI coding assistants like autocomplete. Type faster, accept suggestions, clean up hallucinations. The AI does tasks. The human does damage control.

This inverts the relationship.

The AI should do the work. The human should design the system that makes the work possible.

We maintain a CLAUDE.md file in every repository. It contains: project architecture, naming conventions, what not to do, links to critical documentation. The AI reads this before every session. No more "let me re-explain the codebase." The context is already there.

But context isn''t enough. Work needs to be bounded.

"Build the authentication system" produces hallucinations. The scope is too large. The AI guesses at requirements, invents patterns, loses coherence halfway through.

"Add JWT token generation to the login endpoint" produces working code. The scope is clear. Success criteria are obvious. The AI can verify its own work.

We decompose every feature into tasks small enough that the AI can hold the entire problem in context. If a task requires referencing code the AI hasn''t seen, the task is too big.

The final piece: persistence across sessions.

AI assistants forget everything between conversations. Every session starts from zero. This forces humans to re-explain context—which they do poorly, inconsistently, and incompletely.

We use Beads, an agent-native issue tracker. It records what was attempted, what succeeded, what''s blocked. The AI reads this at session start. Work continues where it left off.

The pattern: context (CLAUDE.md) + bounded tasks + persistent memory (Beads).

The AI isn''t the bottleneck. The system around it is.', 1767891600000, 'pending', 'GTM Sprint 2 - Educational', 1767275140446, '{"commentLink":"Beads (agent-native task tracking): github.com/anthropics/beads\n\nHow we think about AI-human collaboration: createsomething.io/papers/code-mode-hermeneutic-analysis\n\n#AIEngineering #DeveloperProductivity #ClaudeCode"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_z22arbfeh3p', 'linkedin', 'Most AI implementations fail because they try to replace humans entirely.

We helped Arc reduce manual review time by 73%—by identifying what actually needed human judgment.

Arc was drowning in manual reviews. Every new customer meant 8-12 documents to process, 40+ risk criteria to check, 4.2 hours of manual work. Growing backlog, declining quality. They''d tried RPA tools, offshore teams, generic AI. Nothing stuck.

The breakthrough came from a two-week discovery sprint. We found the 80/20 pattern hiding in plain sight: 80% of applications followed predictable patterns. 20% required genuine human judgment.

Previous solutions failed because they treated all cases the same.

We built a three-layer system:

**Layer 1: Document Intelligence**
Extract data from 47 document variations automatically.

**Layer 2: Risk Assessment Engine**
Rules for compliance, ML for patterns, confidence scores for everything.

**Layer 3: Human-in-the-Loop**
Dashboard for edge cases. Feedback that improves the model.

8 weeks later:
- 4.2 hours → 1.1 hours average processing time
- 73% reduction in manual work
- 94% accuracy on automated assessments
- $180K annual operational savings

But the real metric: the team stopped firefighting and started thinking.

The goal isn''t to remove humans from the loop. It''s to put them where they belong—handling the cases that actually need judgment.

Good AI knows what it doesn''t know. Great AI escalates gracefully.', 1767978000000, 'pending', 'GTM Sprint 2 - Case Study', 1767275140446, '{"commentLink":"Full case study: createsomething.agency/work/arc\n\nIf your operations team scales slower than your customer growth, we''ll map your workflow and identify what actually needs automation. Sometimes the answer is less AI, better placed.\n\ncreatesomething.agency/discover\n\n#AIImplementation #OperationsAutomation #B2BSaaS"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_mdvuka1rbap', 'linkedin', 'Every failed automation project we''ve seen skipped the same step: discovery.

Teams jump straight to building. They assume they understand the problem. Six months later, they''ve automated the wrong thing beautifully.

We start every engagement with a two-week discovery sprint. No code. Just understanding.

Week one: Shadow the work. Watch people do their jobs. Don''t ask what they think they do—observe what they actually do. The gap between these is where the real problems hide.

Week two: Map the patterns. Every workflow has an 80/20 split. 80% of cases follow predictable paths. 20% require genuine judgment. Most automation fails because it treats all cases the same.

What discovery reveals:

The stated problem is rarely the real problem. "We need better reporting" usually means "we don''t trust our data." "We need automation" usually means "we have too many manual handoffs."

The obvious solution is rarely the right solution. Arc thought they needed faster document processing. They actually needed to stop processing 80% of documents entirely—they were low-risk and could auto-approve.

The ROI is in the edges. The biggest wins come from eliminating work, not accelerating it. If discovery reveals that half your process shouldn''t exist, you just saved more than any automation could.

Two weeks of understanding beats six months of building the wrong thing.', 1768237200000, 'pending', 'GTM Sprint 2 - Methodology', 1767275140446, '{"commentLink":"Start with a discovery sprint: createsomething.agency/discover\n\nWe''ll tell you what''s actually worth automating—and what isn''t.\n\n#ProcessImprovement #Automation #OperationsExcellence"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_aqbqc41dsyr', 'linkedin', 'Most automation fails because it adds complexity.

We helped Kickstand go from 155 scripts to 13. Here''s what happened when we subtracted instead of added.

The symptoms were familiar: 155 active scripts scattered across the codebase. 30 TypeScript errors blocking development. Health score 6.2/10. Documentation pointing to three different deployment targets. Every "quick fix" had become permanent debt.

Our audit revealed the pattern hiding in plain sight: 142 scripts were variations of 13 core patterns. Two parallel implementations (Node.js and Workers) doing the same thing. 3,200 lines of duplicated code across 8 services.

We didn''t optimize 155 scripts. We identified the 13 that mattered.

The Subtractive Triad in action:

**Level 1 (DRY):** "Have I built this before?" Action: Unify.
**Level 2 (Rams):** "Does this earn its existence?" Action: Remove.
**Level 3 (Heidegger):** "Does this serve the whole?" Action: Reconnect.

Results:
- 155 to 13 scripts (92% reduction)
- Health score: 6.2 to 9.2 (48% improvement)
- 30 to 0 TypeScript errors

But the real win: they can reason about their system now.

"Less, but better" is not minimalism for aesthetics. It''s operational clarity. You can''t scale what you can''t understand. You can''t maintain what you can''t see.

Dieter Rams said it first. We applied it to code.', 1768323600000, 'pending', 'GTM Sprint 1', 1767275140446, '{"commentLink":"Full case study with methodology breakdown: createsomething.agency/work/kickstand\n\n#SoftwareArchitecture #TechnicalDebt #Automation"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_iq2o6icf1r', 'linkedin', 'Most code reviews ask "does it work?"

The better question: "should it exist?"

Every team accumulates features no one uses, abstractions that obscure, patterns that made sense once. We call this "complexity debt." It compounds faster than technical debt.

The solution isn''t better organization. It''s disciplined subtraction.

The Subtractive Triad: three questions, applied in order.

**Level 1 (DRY):** "Have I built this before?"
Action: Unify.

**Level 2 (Rams):** "Does this earn its existence?"
Action: Remove.

**Level 3 (Heidegger):** "Does this serve the whole?"
Action: Reconnect.

Each level catches what the previous one missed.

Example: "Add dark mode to user profiles."

Level 1: Check existing code. Settings page already has ThemeProvider. Reuse it.
Level 2: But wait—Settings already has a theme toggle. Adding another to profiles doesn''t earn its existence.

Rejected at Level 2. We never even needed Level 3.

Most features fail by Level 2. That''s the point.

"Less, but better" isn''t minimalism for aesthetics. It''s operational clarity. Dieter Rams said it first. We applied it to code.', 1768410000000, 'pending', 'GTM Sprint 2 - Methodology', 1767275140446, '{"commentLink":"The full framework: createsomething.ltd/ethos\n\n#SoftwareArchitecture #CleanCode #TechnicalDebt"}');
INSERT INTO social_posts (id, platform, content, scheduled_for, status, campaign, created_at, metadata) VALUES ('sp_svgtmec6mwr', 'linkedin', 'Every codebase we audited this year had the same disease: additions without subtractions.

Features added "for flexibility" that no one used. Abstractions layered on abstractions. Scripts that solved yesterday''s problem, never removed. The symptom is always the same: the team can no longer reason about their own system.

One client had 155 automation scripts. We didn''t optimize them. We asked: which ones actually run? Which ones produce value? The answer: 13. The other 142 were variations, duplicates, or artifacts from problems that no longer existed.

92% of the codebase was noise masquerading as progress.

This isn''t unusual. It''s the default. Systems grow by addition. Subtraction requires intention.

Dieter Rams built his career on "weniger, aber besser"—less, but better. He wasn''t talking about minimalism as aesthetic. He meant: every element must earn its existence. What doesn''t contribute, detracts.

The same principle applies to code. To organizations. To strategy.

The hardest engineering decision isn''t what to build. It''s what to delete. Deletion feels like loss. But a system you can''t understand is already lost.

We''re entering a year where AI will make it trivially easy to add more code, more features, more complexity. The teams that win won''t be the ones who ship fastest. They''ll be the ones disciplined enough to subtract.

The question for 2026 isn''t "what should we build?"

It''s "what should we stop doing?"', 1768496400000, 'pending', 'GTM Sprint 2 - Thought Leadership', 1767275140446, '{"commentLink":"The framework we use for disciplined subtraction: createsomething.ltd/ethos\n\n#SoftwareEngineering #Leadership #TechnicalDebt"}');